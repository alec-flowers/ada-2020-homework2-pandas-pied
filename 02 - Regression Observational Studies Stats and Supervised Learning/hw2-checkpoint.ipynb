{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 (HW2)\n",
    "\n",
    "---\n",
    "By the end of this homework we expect you to be able to:\n",
    "1. Preprocess data and make it amenable to statistical analysis and machine learning models;\n",
    "2. Train and test out-of-the-box machine learning models in Python;\n",
    "3. Carry out statistical hypothesis testing;\n",
    "4. Carry out simple multivariate regression analyses;\n",
    "5. Use techniques to control for covariates;\n",
    "---\n",
    "\n",
    "## Important Dates\n",
    "\n",
    "- Homework release: Fri 6 Nov 2020\n",
    "- **Homework due**: Fri 20 Nov 2020, 23:59\n",
    "- Grade release: Mon 30 Nov 2020\n",
    "\n",
    "---\n",
    "\n",
    "##  Some rules\n",
    "\n",
    "1. You are allowed to use any built-in Python library that comes with Anaconda. If you want to use an external library, you have to justify your choice.\n",
    "2. Make sure you use the data folder provided in the repository in **read-only** mode.\n",
    "3. Be sure to provide a textual description of your thought process, the assumptions you made, the solution you implemented, and explanations for your answers. A notebook that only has code cells will not suffice.\n",
    "4. For questions containing the **/Discuss:/** prefix, answer not with code, but with a textual explanation (in markdown).\n",
    "5. Back up any hypotheses and claims with data, since this is an important aspect of the course.\n",
    "6. Please write all your comments in English, and use meaningful variable names in your code. Your repo should have a single notebook (plus the required data files) in the master branch. If there are multiple notebooks present, we will **strictly** not grade anything.\n",
    "7. Also, be sure to hand in a fully-run and evaluated notebook. We will not run your notebook for you, we will grade it as is, which means that only the results contained in your evaluated code cells will be considered, and we will not see the results in unevaluated code cells. In order to check whether everything looks as intended, you can check the rendered notebook on the GitHub website once you have pushed your solution there.\n",
    "8. Make sure to print results or dataframes that confirm you have properly addressed the task.\n",
    "9. Lastly, the grading is done in the *double blind* mode, i.e., the TAs grades an anonymized version of your notebook, so make sure that your notebook **neither has your team name nor the names of the members**.\n",
    "\n",
    "## Context\n",
    "\n",
    "Publishing papers is a big part of the lives of [Ph.D. students](http://phdcomics.com/comics/archive.php?comicid=154), [post-docs](http://phdcomics.com/comics/archive.php?comicid=1744) and [professors](http://phdcomics.com/comics/archive.php?comicid=1051). \n",
    "In Computer Science, publishing happens mostly in conferences. What follows is a slight simplification of how these conferences decide which papers to accept and which papers to reject.\n",
    " \n",
    "Every year, scholars submit papers to prestigious conferences. The papers are then assigned to reviewers (usually around 3), who are other people from the same research community (respect thy neighbor!). Each reviewer weighs in on whether they believe the papers they were assigned are good or bad, and write a review, often along with a score (e.g. +3 Strong Accept, +2 Accept, +1 Weak Accept, 0 Borderline, …,  -3 Strong Reject). Then, in the end, \"special\" reviewers called, \"Area Chairs\" analyze all the reviews that were written for the same paper and decide what gets accepted and what gets rejected. Importantly, throughout this whole dance, reviewers and authors are anonymous. When you're reviewing a paper, you do not know who wrote it. And when you receive the review, you don't know who reviewed it. Because of that, we call this a double-blind reviewing process.\n",
    "\n",
    "An interesting development that has evolved in recent years is the rise of pre-prints. In previous times, researchers often exposed their research to the world only after it had been peer-reviewed and published in a conference or a journal. But recently researchers are much keener to let their ideas out into the world as soon as possible, and they publish their research before it has been approved to any conference or journal, by posting the research on so-called pre-print servers. The most common pre-print server for Computer Science, Physics, and Maths is called [arXiv](https://arxiv.org/), for Biology, an increasingly popular one is [bioRxiv](https://www.biorxiv.org/), for Psychology [psyArXiv](https://psyarxiv.com/) (they are not very creative with the names). Notice that pre-prints and peer-review are not mutually exclusive, in fact, usually, you publish your pre-print, and then you try to publish your work in a peer-reviewed setting.\n",
    "\n",
    "Overall, publishing pre-prints has many benefits. They make science more accessible and hasten the circulation of important results in the academic community. However, a big issue brought forth with pre-prints is that they often break the anonymity in the double-blind reviewing process. For instance, in machine learning, since most papers are published as pre-prints, it is often easy to figure out if the paper you are reviewing is from a famous researcher or a big company with prominent research scientists. From critics' viewpoint, knowing the authors of the papers you are reviewing can bias your reviews. If you know that a given author is famous, you'd be more inclined to take his or her word for granted. If the author is from an institution you’ve never heard about, you are more likely to doubt his or her findings.\n",
    "\n",
    "In this homework, we will take a data-driven deep dive into the world of academic publishing. Can you use your freshly acquired data-science skills to predict which papers are going to make the cut? Are your data analysis skills sharp enough to figure out whether the aforementioned concerns about pre-print issues are justified?\n",
    "\n",
    "\n",
    "## The data\n",
    "\n",
    "The data, whose source has been *\"double-blinded\"* from you, and which has been simplified a bit for the assignment, contains information about submissions to a prestigious machine learning conference called ICLR (pronounced “I-clear”). You can find the dataframe in the git repo for the homework (`./data/dataset_final.csv`). We provide a brief description of the fields you will encounter.\n",
    "\n",
    "- `year`: year the paper has been submitted to ICLR. Notice that we provide data for three years, 2018, 2019, and 2020.\n",
    "\n",
    "\n",
    "- `paper`: title of the paper.\n",
    "\n",
    "\n",
    "- `authors`: names of the authors separated by ;.\n",
    "\n",
    "\n",
    "- `ratings`: mean rating given to the paper by the reviewers.\n",
    "\n",
    "\n",
    "- `decisions`: either Accept if the paper was accepted, or Reject otherwise.\n",
    "\n",
    "\n",
    "- `institution`: institutions for each of the authors, separated by ;.\n",
    "\n",
    "\n",
    "- `csranking`: ranking of the institutions according to csrankings. The better the institution, the better the rank. Notice that, if a paper has more than 1 author, this field will contain multiple values, separated by ;. For institutions that are not in csrankings, the value will be -1.\n",
    "\n",
    "\n",
    "- `categories`: topical categories of the paper. Each number corresponds to a different category: (1) Theory, (2) Computer Vision, (3) Natural Language Processing, (4) Adversarial ML, (5) Generative Modeling, (7) Fairness, (8) Generalization, (9) Optimization, (10) Graphs, (11) Bayesian Methods, (0) Others. A paper may belong to multiple categories, separated by ;.\n",
    "\n",
    "\n",
    "- `authors_citations`: number of citations of each one of the authors, separated by ;.\n",
    "\n",
    "\n",
    "- `authors_publications`: number of publications by each one of the authors, separated by ;.\n",
    "\n",
    "\n",
    "- `authors_hindex`: h-index of each one of the authors, separated by ;. The h-index is an author-level metric that measures both the productivity and citation impact of the publications of a scientist or scholar. It is the maximum value $h$ such that the given author has published $h$ papers that have each been cited at least $h$ times.\n",
    "\n",
    "\n",
    "- `arxiv`: whether the paper was spotted in a pre-print server around the submission period.\n",
    "\n",
    "Also, notice that in this dataframe, when some piece of data was not available, -1 will be used as the value. For example, companies aren't a part of csrankings, so for people who work in big companies, the values are -1 in the field csranking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection, linear_model, metrics, ensemble\n",
    "import statsmodels.api as sm\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import scipy\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/dataset_final.csv'\n",
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4256.000000</td>\n",
       "      <td>4256.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019.291353</td>\n",
       "      <td>4.963381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.766931</td>\n",
       "      <td>1.492028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2018.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              year      ratings\n",
       "count  4256.000000  4256.000000\n",
       "mean   2019.291353     4.963381\n",
       "std       0.766931     1.492028\n",
       "min    2018.000000     1.000000\n",
       "25%    2019.000000     4.000000\n",
       "50%    2019.000000     5.000000\n",
       "75%    2020.000000     6.000000\n",
       "max    2020.000000     9.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Step 1:_ Predicting paper ratings\n",
    "\n",
    "The first part of this homework poses a simple question: Can you predict the ratings the paper will receive given attributes related to its authors? To answer this question, we will build an ML pipeline from scratch, preprocessing the data, training a regression model, and then evaluating it.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Task 1.1\n",
    "\n",
    "Some of the fields in the data are not very amenable to serve as input to traditional machine learning algorithms. Namely, we have some fields for which there are a varying number of values (e.g. papers with 3 authors will have 3 values in the `author_citations` field, papers with 5 authors will have 5).\n",
    "\n",
    "\n",
    "Your first task is to perform some feature engineering and derive unique values for each paper which you will be able to use in your ML model. \n",
    "More specifically, you must:\n",
    "\n",
    "1. Create 3 new fields in the dataframe corresponding to the median value of the number of citations per author, the number of publications per author, and the h-index per author. So for instance, for the row `authors_publications`, you will create an additional column, e.g. `authors_publications_median`, containing the median number of publications per author in each paper.\n",
    "2. Create another field entitled `reputation` capturing how famous the last author of the paper is. Notice that the last author of the paper is usually the most senior person involved in the project. This field should equal $\\log_{10}\\Big(\\frac{\\#citations}{\\#publications} + 1\\Big)$. Notice that each author in the dataset has at least 1 publication, so you don't risk dividing by 0.\n",
    "3. Create two fields called `has_top_company` and `has_top_institution`. The field `has_top_company` equals 1 if the article contains an author in the following list of companies `[\"Facebook\", \"Google\", \"Microsoft\", \"Deepmind\"]`, and 0 otherwise. The field `has_top_institution` equals 1 if the article contains an author in the top 10 institutions according to CSRankings.\n",
    "4. **Discuss:** How did you handle -1 values in item 1.1.1? Justify your approach.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string to list of the following columns\n",
    "\n",
    "df['authors_citations'] = df['authors_citations'].str.split(';').apply(lambda x: list(map(int,x)))\n",
    "df['authors_publications'] = df['authors_publications'].str.split(';').apply(lambda x: list(map(int,x)))\n",
    "df['authors_hindex'] = df['authors_hindex'].str.split(';').apply(lambda x: list(map(int,x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number -1 in authors_citations: 96\n",
      "Number -1 in authors_publications: 96\n",
      "Number -1 in authors_hindex: 96\n"
     ]
    }
   ],
   "source": [
    "# study on -1 values\n",
    "\n",
    "print(f\"Number -1 in authors_citations: {sum(df['authors_citations'].apply(lambda x: -1 in x))}\")\n",
    "print(f\"Number -1 in authors_publications: {sum(df['authors_publications'].apply(lambda x: -1 in x))}\")\n",
    "print(f\"Number -1 in authors_hindex: {sum(df['authors_hindex'].apply(lambda x: -1 in x))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional column with the median\n",
    "\n",
    "df['authors_citations_median'] = df['authors_citations'].apply(lambda x: np.median(x))\n",
    "df['authors_publications_median'] = df['authors_publications'].apply(lambda x: np.median(x))\n",
    "df['authors_hindex_median'] = df['authors_hindex'].apply(lambda x: np.median(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new column reputation capturing how famous the last author of the paper is\n",
    "\n",
    "df['reputation'] = df.apply(lambda x: np.log10(x['authors_citations'][-1] / x['authors_publications'][-1] + 1), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two new column 'has_top_company' if the article contains an author in TOP_COMPAGNY\n",
    "\n",
    "TOP_COMPANY = '|'.join([\"Facebook\", \"Google\", \"Microsoft\", \"Deepmind\"])\n",
    "df['has_top_company'] = df['institution'].str.contains(TOP_COMPANY).astype(int)\n",
    "\n",
    "# 'has_top_institution' if the article contains an author in the top 10 institutions\n",
    "#  according to CSRankings.\n",
    "\n",
    "df['csranking'] = df['csranking'].str.split(';').apply(lambda x: list(map(int,x)))\n",
    "\n",
    "def is_inbetween(x, min_ = 0, max_ = 10):\n",
    "    'Return true if values is inbetween 0 and 10, otherwise false'\n",
    "    \n",
    "    if (x >= min_) & (x <= 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df['has_top_institution'] = df['csranking'].apply(lambda x: any([is_inbetween(i) for i in x])).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1.4**\n",
    "#### **Discussion :** \n",
    "**How did you handle -1 values in item 1.1.1? Justify your approach.**\n",
    "\n",
    "The normal options to deal with null values (-1) are to drop these values or to impute something for the value. In 1.1.1 we are creating new features that takes the median of a group of values, with some of these values being null. Having a null value for these scores means that we could not find available data. In this case I think it is ok to leave the -1 as in and to pull the median value of the group down should there be a null value. There is only 1 paper whose medians are actually -1 which means they are the only group with a majority of nulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>paper</th>\n",
       "      <th>authors</th>\n",
       "      <th>ratings</th>\n",
       "      <th>decisions</th>\n",
       "      <th>institution</th>\n",
       "      <th>csranking</th>\n",
       "      <th>categories</th>\n",
       "      <th>authors_citations</th>\n",
       "      <th>authors_publications</th>\n",
       "      <th>authors_hindex</th>\n",
       "      <th>arxiv</th>\n",
       "      <th>authors_citations_median</th>\n",
       "      <th>authors_publications_median</th>\n",
       "      <th>authors_hindex_median</th>\n",
       "      <th>reputation</th>\n",
       "      <th>has_top_company</th>\n",
       "      <th>has_top_institution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>2020</td>\n",
       "      <td>CNAS: Channel-Level Neural Architecture Search</td>\n",
       "      <td>Heechul Lim;Min-Soo Kim;Jinjun Xiong</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Gwangju Institute of Science and Technology;Gw...</td>\n",
       "      <td>[-1, -1, -1]</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, 1775]</td>\n",
       "      <td>[-1, -1, 171]</td>\n",
       "      <td>[-1, -1, 23]</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.056147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                                           paper  \\\n",
       "4238  2020  CNAS: Channel-Level Neural Architecture Search   \n",
       "\n",
       "                                   authors  ratings decisions  \\\n",
       "4238  Heechul Lim;Min-Soo Kim;Jinjun Xiong      2.5    Reject   \n",
       "\n",
       "                                            institution     csranking  \\\n",
       "4238  Gwangju Institute of Science and Technology;Gw...  [-1, -1, -1]   \n",
       "\n",
       "     categories authors_citations authors_publications authors_hindex  arxiv  \\\n",
       "4238          0    [-1, -1, 1775]        [-1, -1, 171]   [-1, -1, 23]  False   \n",
       "\n",
       "      authors_citations_median  authors_publications_median  \\\n",
       "4238                      -1.0                         -1.0   \n",
       "\n",
       "      authors_hindex_median  reputation  has_top_company  has_top_institution  \n",
       "4238                   -1.0    1.056147                0                    0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Team with a -1 value\n",
    "\n",
    "df[df['authors_hindex_median'] == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "\n",
    "Now that you have cleaned up your data, your next task is to divide your data into a training set and a testing set. You should do this in two ways:\n",
    "\n",
    "1. First, do it randomly. Split the data into a training set (70%) and a testing set (30%). We refer to these as \"random split\" in the subsequent tasks.\n",
    "2. Second, do it longitudinally. Use the data from 2018 and 2019 for the training set, and the data from 2020 as the testing set. We refer to these as \"longitudinal split\" in the subsequent tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_randomly(X,y,test_size,random_state = 1):\n",
    "    '''\n",
    "    Input: features dataset X , predict feature y , size of the test df test_size\n",
    "    Output: respectively X and y  are splitting randomly btw train and test dataset\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,test_size = test_size, random_state = random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_longitudinally(X,y,predict_feature):\n",
    "    '''\n",
    "    Input: features dataset X , predict feature y \n",
    "    Output: X_ltrain,y_ltrain made using 2018,2019 X_ltest,y_ltest made from 2020 dataset\n",
    "    '''\n",
    "    X_ltrain = df[df['year'] <= 2019].drop(predict_feature, axis = 1)\n",
    "    y_ltrain = df[df['year'] <= 2019][predict_feature]\n",
    "    \n",
    "    X_ltest = df[df['year'] == 2020].drop(predict_feature, axis = 1)\n",
    "    y_ltest = df[df['year'] == 2020][predict_feature]\n",
    "\n",
    "\n",
    "    return X_ltrain, X_ltest, y_ltrain, y_ltest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "Build a Linear Regression model (use [sklearn](https://scikit-learn.org/stable/)) that predicts the score of a paper (which is in the variable ratings in the dataframe). Train it and test it using the split you previously defined. Your model should use as features:\n",
    "- Median values for the number of author citations, publications and h-indexes, as calculated in Task 1.1.1.\n",
    "\n",
    "- `reputation` of the last author, as calculated in Task 1.1.2.\n",
    "\n",
    "For the two scenarios above (random split and longitudinal split):\n",
    "\n",
    "1. Report the model $R^2$ in each case (for the testing set).\n",
    "2. **Discuss:** Hypothesize a reason why the results are different. Additionally, interpret the $R^2$ value for the longitudinal split. How can it be negative?\n",
    "3. **From now onwards (in this task and the following ones), consider only the random split.** For a given entry $X$ your model outputs a predicted score $Y'$. The difference between the real score $Y$ and the predicted score $Y'$ is called the \"residual\". Plot the distribution of your residuals for the test set. Using this distribution, estimate what is the probability that your prediction is off by more than 2-points? Provide bootstrapped confidence intervals for your answer.\n",
    "4. **Discuss:** Identify three additional features that are already computed in your dataframe and that could boost your model's predictive performance. You are not allowed to use the variable `decisions` as an input here. Before running any experiments, discuss why each of these features might add valuable information to your model.\n",
    "5. Report the $R^2$ (for the test set) for a newly trained model with these additional features. Please note that you do not need to improve the model performance to be successful in this task!\n",
    "\n",
    "**Hint**: [Metrics!](https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of the feature use for predicting\n",
    "FEATURES = ['authors_hindex_median','authors_publications_median','authors_citations_median','reputation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scenario 1 - Random**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linear_regression(df, features):\n",
    "    '''\n",
    "    Input: full dataset df , features used for regression\n",
    "    Output: y_test of our test df, our prediction 'pred' and R2 score\n",
    "    '''\n",
    "    y = df['ratings']\n",
    "    X = df.drop('ratings',axis = 1)\n",
    "    X = df[features]\n",
    "    \n",
    "    # splitting randomly\n",
    "    X_train, X_test, y_train, y_test = split_data_randomly(X,y,test_size = .3)\n",
    "\n",
    "    #linear regression\n",
    "    reg_random = linear_model.LinearRegression().fit(X_train,y_train)\n",
    "    pred = reg_random.predict(X_test)\n",
    "    r2 = metrics.r2_score(y_test,pred)\n",
    "    \n",
    "    return y_test, pred, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 =  0.08749306772992715\n"
     ]
    }
   ],
   "source": [
    "#linear regression \n",
    "y_test, pred_random, r2 = linear_regression(df, FEATURES)\n",
    "print('R2 = ',r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scenario 2 - Longitudinal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 =  -0.2388121312091822\n"
     ]
    }
   ],
   "source": [
    "y = df['ratings']\n",
    "X = df.drop('ratings',axis = 1)\n",
    "# splitting longitudinally\n",
    "X_ltrain, X_ltest, y_ltrain, y_ltest = split_data_longitudinally(X,y,predict_feature = 'ratings')\n",
    "\n",
    "# choice of feature for prediction\n",
    "X_ltrain = X_ltrain[FEATURES]\n",
    "X_ltest = X_ltest[FEATURES]\n",
    "\n",
    "#linear regression\n",
    "reg_long = linear_model.LinearRegression().fit(X_ltrain,y_ltrain)\n",
    "pred_long = reg_long.predict(X_ltest)\n",
    "r2= metrics.r2_score(y_ltest,pred_long)\n",
    "print('R2 = ',r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Discussion:** \n",
    "\n",
    "**Hypothesize a reason why the results are different. Additionally, interpret the $R^2$ value for the longitudinal split. How can it be negative?**\n",
    "\n",
    "The formula for $R^2$ = 1 - $\\big(\\frac{SSR}{TSS}\\big)$ = 1 - $\\big(\\frac{\\sum_{}{}e_i^{2}}{\\sum_{}{}y_i - \\bar{Y}}\\big)$\n",
    "\n",
    "The $R^2$ is the proportion of the variance in the dependent variable that is predictable from the independent variables. We know that a baseline model which always just predicts the mean , $\\bar{Y}$ of the data has an R^2 of 0. A model with a negative prediction score has a negative $R^2$.\n",
    "\n",
    "It already looks like in both cases we have poor 0.087 and terrible -0.239 $R^2$ scores which means that a linear combination of our features does not do a great job of fitting to our data.\n",
    "\n",
    "For 0.087 if we look at our formula this is saying our sum of squared error for the residuals is almost as large as the total sum of squared error. Having large errors for residuals is bad as this means our data is far away from our prediction line. We are only doing slightly better than if we just fit a line representing the mean to the data. \n",
    "\n",
    "A -0.239 means that our residual error is actually larger than the variance. We are actually doing worse than just fitting to the mean of the data. This can happen when our regression line is going opposite of trend in the data and means that our model with these featues and a longitutidinal test set is very poor. \n",
    "\n",
    "Interpreting this, clearly historical data on ratings based on these features actually makes us predict worse. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.3** \n",
    "#### Residual plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From now onwards (in this task and the following ones), consider only the random split.** For a given entry $X$ your model outputs a predicted score $Y'$. The difference between the real score $Y$ and the predicted score $Y'$ is called the \"residual\". Plot the distribution of your residuals for the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme();\n",
    "residual = y_test - pred_random\n",
    "plt.hist(residual, bins = 30, density = True)\n",
    "plt.title('Residual distribution for random split \\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(residual, kind=\"kde\")\n",
    "plt.title('Residual distribution for random split \\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(residual, kind=\"ecdf\")\n",
    "plt.title('Empirical cumulative distribution functions of residual for random split \\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(residual, bins = 30, density = True, cumulative = 'True', histtype = 'step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this distribution, estimate what is the probability that your prediction is off by more than 2-points? Provide bootstrapped confidence intervals for your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that our prediction is off by more than 2 point is about 15.897. %\n"
     ]
    }
   ],
   "source": [
    "prob_off = ((residual > 2).sum() + (residual < -2).sum()) / residual.count()\n",
    "print('The probability that our prediction is off by more than 2 point is about {}. %'.format(round(prob_off * 100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostrap_CI(df, features, max_iter, interval_size):\n",
    "    '''\n",
    "    Input : full dataset df , features used for regression , number of iteration max_iter\n",
    "    Output : confidence intervals using the empirical distribution with ----- TO DO ----- as an interval size.\n",
    "    '''\n",
    "    y = df['ratings']\n",
    "    X = df.drop('ratings',axis=1)\n",
    "    X = X[features]\n",
    "    ci = []\n",
    "    for i in range(max_iter):\n",
    "        X_train, X_test, y_train, y_test = split_data_randomly(X,y,test_size=.5,random_state=None)\n",
    "        reg_random = linear_model.LinearRegression().fit(X_train,y_train)\n",
    "        pred = reg_random.predict(X_test)\n",
    "\n",
    "        residual = y_test - pred\n",
    "        ci.append(((residual > 2).sum() + (residual < -2).sum()) / residual.count())\n",
    "    \n",
    "    confidence_interval = sorted(ci)\n",
    "    \n",
    "    return [confidence_interval[interval_size],confidence_interval[-interval_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His corresponded bootstrapped confidence intervals is [0.15084586466165414, 0.17199248120300753]. \n"
     ]
    }
   ],
   "source": [
    "max_iter = 1000\n",
    "ci = boostrap_CI(df, FEATURES, max_iter,int(.025*max_iter))\n",
    "print('His corresponded bootstrapped confidence intervals is {}. '.format(ci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.4**\n",
    "**Discuss:** Identify three additional features that are already computed in your dataframe and that could boost your model's predictive performance. You are not allowed to use the variable `decisions` as an input here. Before running any experiments, discuss why each of these features might add valuable information to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three features that we decide to use are:\n",
    "1) Arvix  turn into boolean vs not boolean. This feature will be helpful because at the end we want to know if the Arvix influence on the decision.\n",
    "\n",
    "2) Year as one hot encode. We saw with the longitudinal data it looks like there isn't as much predictivate power from year to year. So would be a good feature to add to add robustness to our model\n",
    "\n",
    "3) Reputation of first author. As the other's author influence the ratings, as well. We add the reputation of the first author in our model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first feature\n",
    "one_hot = pd.get_dummies(df['arxiv'], prefix = 'arxiv')\n",
    "df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second feature\n",
    "one_hot = pd.get_dummies(df['year'], prefix = 'year')\n",
    "df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third feature\n",
    "df['reputation_one'] = df.apply(lambda x: np.log10(x['authors_citations'][0]\n",
    "                                                   /x['authors_publications'][0] + 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_FEATURES = ['authors_hindex_median','authors_publications_median',\n",
    "                'authors_citations_median','reputation','arxiv_False','arxiv_True',\n",
    "                'year_2018', 'year_2019', 'year_2020', 'reputation_one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With our new set of features, we obtain a R2 equal to 0.217.\n"
     ]
    }
   ],
   "source": [
    "y_test, pred, r2 = linear_regression(df, NEW_FEATURES)\n",
    "print('With our new set of features, we obtain a R2 equal to {}.'.format(round(r2,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then observe that by adding our new features the R2 found by linear regression grows from 0.087 to 0.217, which shows that our choice of feature helps have a better prediciton of our dataset. We can add that this result doesn't guarantee that all the new features add valuable information to our model. By example, one of this features could be really great and add a lot of information to our model, while the others doesn't. So without any other experiment we can only conculde that those features as a group add valuable information to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4\n",
    "\n",
    "Experiment with training a different regressor, a [Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/ensemble.html?highlight=xgboost#gradient-boosting). This regressor is analogous to the Gradient Boosting Classifier that you have seen in class. This model performs extremely well for a variety of tasks and is often used in machine learning competitions for tabular data (e.g., on [Kaggle](www.kaggle.com)). You must:\n",
    "\n",
    "1. Train a Gradient Boosting Regressor without specifying any parameters, e.g. `GradientBoostingRegressor().fit(X, y)`, and report its $R^2$ on the testing set. Your model should again use as features:\n",
    "    - Median values for the number of author citations, publications and h-indexes as calculated in Task 1.1.1.\n",
    "    - `reputation` of the last author, as calculated in Task 1.1.2.\n",
    "2. Create an additional feature called $crazy$, which is derived as follows. If the score  of the paper, $ratings$, is bigger than 4.96, then $crazy = 9 - ratings$, otherwise, $crazy = - ratings$. Train a Gradient Boosting Regressor to predict paper scores using only $crazy$ as a feature. Additionally, train a Linear Regression model to predict paper scores using only $crazy$ as a feature. Report the $R^2$ in the testing set.\n",
    "3. **Discuss:** Why does the Gradient Boosting Regressor perform so much better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boost_regression(df, features):\n",
    "    '''\n",
    "    Input: full dataset df , features used for regression\n",
    "    Output: y_test of our test df, our prediction 'pred' and R2 score\n",
    "    '''\n",
    "    \n",
    "    y = df['ratings']\n",
    "    X = df.drop('ratings',axis=1)\n",
    "    X = df[features]\n",
    "    #splitting\n",
    "    X_train, X_test, y_train, y_test = split_data_randomly(X,y,test_size = .3)\n",
    "    \n",
    "    #Gradient Boosting Regression\n",
    "    reg_random = ensemble.GradientBoostingRegressor().fit(X_train,y_train)\n",
    "    pred = reg_random.predict(X_test)\n",
    "    r2 = metrics.r2_score(y_test,pred)\n",
    "    \n",
    "    return y_test, pred, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10526660773566376"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Regression with FEATURES used in Task 1.3\n",
    "\n",
    "y_test, pred, r2 = gradient_boost_regression(df, FEATURES)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in Task 1.3 with the same feature we calculate a R2 of 0.087 for the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4.2**\n",
    "Create an additional feature called $crazy$, which is derived as follows. If the score  of the paper, $ratings$, is bigger than 4.96, then $crazy = 9 - ratings$, otherwise, $crazy = - ratings$. Train a Gradient Boosting Regressor to predict paper scores using only $crazy$ as a feature. Additionally, train a Linear Regression model to predict paper scores using only $crazy$ as a feature. Report the $R^2$ in the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_feature(x):\n",
    "    '''\n",
    "    Input: a rating of an article \n",
    "    Output: the corresponding crazy score\n",
    "    '''\n",
    "    if x >4.96:\n",
    "        crazy = 9 - x\n",
    "    else:\n",
    "        crazy = -x\n",
    "    return crazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the feature crazy\n",
    "\n",
    "df['crazy'] = df['ratings'].apply(crazy_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 =  0.9999900346272116\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting Regression with only 'crazy' as a feature.\n",
    "\n",
    "y_test, pred, r2 = gradient_boost_regression(df, ['crazy'])\n",
    "print('R2 = ',r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 =  0.4216318406900581\n"
     ]
    }
   ],
   "source": [
    "# Linear regression with only 'crazy' as a feature.\n",
    "\n",
    "y_test, pred, r2 = linear_regression(df, ['crazy'])\n",
    "print('R2 = ',r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4.3**\n",
    "#### **Discussion:** \n",
    "Why does the Gradient Boosting Regressor perform so much better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In task 1.4.2, we observe a R2 more than two times better when we use the gradient boosting regression than the linear regression. This can be explain because the gradient boosting regression is an boosting ensemble method. The ensemble method represents a ensemble of weak predictive models that we train an then combine their result to make a better learner. The boosting is a type of ensemble methods which trains the model sequentially on a multitude of step. At each step, we train a new learner that puts emphasis on the points that the previous learner got wrong. So it basically corrects the previous learner at every steps, while the linear Regression fits  directly a linear model without boosting it .\n",
    "\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Why the previous one is not why more bigger then ?\n",
    "\n",
    "--------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5\n",
    "\n",
    "Complex models often have several hyper-parameters. \n",
    "To obtain the best results, it is common-place to use a cross-validation set-up in your training data to find the best hyper-parameters, and then use it for the test set.\n",
    "\n",
    "\n",
    "1. Write modular code (i.e., a function) to divide your training data into $N$ folds and perform cross-validation.\n",
    "2.  Experiment tuning two hyper-parameters of the Gradient Boosting Regressor: `n_estimators` and `learning_rate`.\n",
    "For each possible combination of the two hyper-parameters (see below for the range of values that you should try for each hyper-parameter), train your model in a cross-validation setup with $N$=20. Report the mean $R^2$ along with the 90% CI for the 18 scenarios. Notice that you can calculate the 90% CI in a bootstrap-like fashion.\n",
    "    - `n_estimators`$ \\in  \\{ 50, 75, 100, 150, 200, 250\\}$\n",
    "    - `learning_rate`$ \\in  \\{ 0.1, 0.05, 0.01\\}$.\n",
    "3. With the best hyper-parameters obtained, train your model with the entire training set and report the $R^2$ on the testing set.\n",
    "4. **Discuss:** Why don't we tune the hyper-parameters in the testing set instead of doing cross-validation in the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.5.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_N_fold(X,y, N_fold, seed = 1):\n",
    "    '''\n",
    "    Input: dataset X , prediction feature y , number of folds N_fold\n",
    "    Output: table with all the index representing the N-folds\n",
    "    '''\n",
    "    N_fold_df = pd.DataFrame()\n",
    "    np.random.seed(seed) # Initializing the random \n",
    "    permut = np.random.permutation(X.shape[0]) #  permutation of the indexes of the dataset\n",
    "    # permute the data\n",
    "    X = X.iloc[permut,:] \n",
    "    y = y[permut]\n",
    "    indices_N = np.split(permut[:20*int(X_train.shape[0]/20)],N_fold) # Split the dataset maximal part of the data set\n",
    "    # divided by 20 into k mini dataset\n",
    "    return pd.DataFrame(indices_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 212)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['ratings']\n",
    "X = df.drop('ratings',axis=1)\n",
    "X = df[FEATURES]\n",
    "N_fold_index_df = build_N_fold(X,y, N_fold=20, seed = 1)\n",
    "N_fold_index_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1050</td>\n",
       "      <td>2858</td>\n",
       "      <td>219</td>\n",
       "      <td>1636</td>\n",
       "      <td>3178</td>\n",
       "      <td>3224</td>\n",
       "      <td>2838</td>\n",
       "      <td>1336</td>\n",
       "      <td>3489</td>\n",
       "      <td>3540</td>\n",
       "      <td>...</td>\n",
       "      <td>906</td>\n",
       "      <td>1448</td>\n",
       "      <td>1768</td>\n",
       "      <td>2861</td>\n",
       "      <td>2296</td>\n",
       "      <td>1333</td>\n",
       "      <td>4189</td>\n",
       "      <td>280</td>\n",
       "      <td>2567</td>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1825</td>\n",
       "      <td>1030</td>\n",
       "      <td>891</td>\n",
       "      <td>710</td>\n",
       "      <td>3020</td>\n",
       "      <td>1185</td>\n",
       "      <td>2494</td>\n",
       "      <td>1156</td>\n",
       "      <td>1832</td>\n",
       "      <td>3067</td>\n",
       "      <td>...</td>\n",
       "      <td>1610</td>\n",
       "      <td>1805</td>\n",
       "      <td>3636</td>\n",
       "      <td>1777</td>\n",
       "      <td>3539</td>\n",
       "      <td>2329</td>\n",
       "      <td>1089</td>\n",
       "      <td>1432</td>\n",
       "      <td>1848</td>\n",
       "      <td>2361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2064</td>\n",
       "      <td>2538</td>\n",
       "      <td>2698</td>\n",
       "      <td>3524</td>\n",
       "      <td>1143</td>\n",
       "      <td>2062</td>\n",
       "      <td>1391</td>\n",
       "      <td>4013</td>\n",
       "      <td>1794</td>\n",
       "      <td>3619</td>\n",
       "      <td>...</td>\n",
       "      <td>3220</td>\n",
       "      <td>2018</td>\n",
       "      <td>1837</td>\n",
       "      <td>2900</td>\n",
       "      <td>487</td>\n",
       "      <td>1895</td>\n",
       "      <td>2313</td>\n",
       "      <td>749</td>\n",
       "      <td>940</td>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5     6     7     8     9    ...   202  \\\n",
       "0  1050  2858   219  1636  3178  3224  2838  1336  3489  3540  ...   906   \n",
       "1  1825  1030   891   710  3020  1185  2494  1156  1832  3067  ...  1610   \n",
       "2  2064  2538  2698  3524  1143  2062  1391  4013  1794  3619  ...  3220   \n",
       "\n",
       "    203   204   205   206   207   208   209   210   211  \n",
       "0  1448  1768  2861  2296  1333  4189   280  2567  1380  \n",
       "1  1805  3636  1777  3539  2329  1089  1432  1848  2361  \n",
       "2  2018  1837  2900   487  1895  2313   749   940  1123  \n",
       "\n",
       "[3 rows x 212 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_fold_index_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.5.2**\n",
    "2.  Experiment tuning two hyper-parameters of the Gradient Boosting Regressor: `n_estimators` and `learning_rate`.\n",
    "For each possible combination of the two hyper-parameters (see below for the range of values that you should try for each hyper-parameter), train your model in a cross-validation setup with $N$=20. Report the mean $R^2$ along with the 90% CI for the 18 scenarios. Notice that you can calculate the 90% CI in a bootstrap-like fashion.\n",
    "    - `n_estimators`$ \\in  \\{ 50, 75, 100, 150, 200, 250\\}$\n",
    "    - `learning_rate`$ \\in  \\{ 0.1, 0.05, 0.01\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of regressor\n",
    "gb = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "# settings\n",
    "parameters = {'n_estimators': [50, 75, 100, 150, 200, 250],\n",
    "              'learning_rate': [.1, .05, .01]}\n",
    "y = df['ratings']\n",
    "X = df.drop('ratings',axis=1)\n",
    "X = df[NEW_FEATURES]\n",
    "\n",
    "# splitting the train and test dataset\n",
    "X_train, X_test, y_train, y_test = split_data_randomly(X,y,test_size = .3, random_state = 1 )\n",
    "\n",
    "# cross k validation using GridsearchCV\n",
    "clf = model_selection.GridSearchCV(gb, parameters, cv = 20, scoring = 'r2')\n",
    "clf.fit(X_train,y_train)\n",
    "best_param = clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters found with cross k validation with a k=20 are {'learning_rate': 0.05, 'n_estimators': 75}.\n"
     ]
    }
   ],
   "source": [
    "print('The best parameters found with cross k validation with a k=20 are {}.'.format(best_param ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split10_test_score</th>\n",
       "      <th>split11_test_score</th>\n",
       "      <th>split12_test_score</th>\n",
       "      <th>split13_test_score</th>\n",
       "      <th>split14_test_score</th>\n",
       "      <th>split15_test_score</th>\n",
       "      <th>split16_test_score</th>\n",
       "      <th>split17_test_score</th>\n",
       "      <th>split18_test_score</th>\n",
       "      <th>split19_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.219620</td>\n",
       "      <td>0.166757</td>\n",
       "      <td>0.231053</td>\n",
       "      <td>0.248799</td>\n",
       "      <td>0.149674</td>\n",
       "      <td>0.223973</td>\n",
       "      <td>0.093073</td>\n",
       "      <td>0.184932</td>\n",
       "      <td>0.254711</td>\n",
       "      <td>0.286211</td>\n",
       "      <td>0.238673</td>\n",
       "      <td>0.177482</td>\n",
       "      <td>0.074305</td>\n",
       "      <td>0.248978</td>\n",
       "      <td>0.291298</td>\n",
       "      <td>0.258237</td>\n",
       "      <td>0.235049</td>\n",
       "      <td>0.169351</td>\n",
       "      <td>0.162488</td>\n",
       "      <td>0.132780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.218892</td>\n",
       "      <td>0.155897</td>\n",
       "      <td>0.218639</td>\n",
       "      <td>0.229594</td>\n",
       "      <td>0.145551</td>\n",
       "      <td>0.225651</td>\n",
       "      <td>0.091252</td>\n",
       "      <td>0.166145</td>\n",
       "      <td>0.249147</td>\n",
       "      <td>0.277019</td>\n",
       "      <td>0.241547</td>\n",
       "      <td>0.176635</td>\n",
       "      <td>0.065291</td>\n",
       "      <td>0.254848</td>\n",
       "      <td>0.296614</td>\n",
       "      <td>0.250831</td>\n",
       "      <td>0.224289</td>\n",
       "      <td>0.174804</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.123704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.216721</td>\n",
       "      <td>0.155043</td>\n",
       "      <td>0.214008</td>\n",
       "      <td>0.211758</td>\n",
       "      <td>0.146979</td>\n",
       "      <td>0.222477</td>\n",
       "      <td>0.093085</td>\n",
       "      <td>0.148650</td>\n",
       "      <td>0.249626</td>\n",
       "      <td>0.271968</td>\n",
       "      <td>0.239734</td>\n",
       "      <td>0.170080</td>\n",
       "      <td>0.054859</td>\n",
       "      <td>0.248724</td>\n",
       "      <td>0.297457</td>\n",
       "      <td>0.243199</td>\n",
       "      <td>0.213202</td>\n",
       "      <td>0.182748</td>\n",
       "      <td>0.162867</td>\n",
       "      <td>0.120160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0           0.219620           0.166757           0.231053           0.248799   \n",
       "1           0.218892           0.155897           0.218639           0.229594   \n",
       "2           0.216721           0.155043           0.214008           0.211758   \n",
       "\n",
       "   split4_test_score  split5_test_score  split6_test_score  split7_test_score  \\\n",
       "0           0.149674           0.223973           0.093073           0.184932   \n",
       "1           0.145551           0.225651           0.091252           0.166145   \n",
       "2           0.146979           0.222477           0.093085           0.148650   \n",
       "\n",
       "   split8_test_score  split9_test_score  split10_test_score  \\\n",
       "0           0.254711           0.286211            0.238673   \n",
       "1           0.249147           0.277019            0.241547   \n",
       "2           0.249626           0.271968            0.239734   \n",
       "\n",
       "   split11_test_score  split12_test_score  split13_test_score  \\\n",
       "0            0.177482            0.074305            0.248978   \n",
       "1            0.176635            0.065291            0.254848   \n",
       "2            0.170080            0.054859            0.248724   \n",
       "\n",
       "   split14_test_score  split15_test_score  split16_test_score  \\\n",
       "0            0.291298            0.258237            0.235049   \n",
       "1            0.296614            0.250831            0.224289   \n",
       "2            0.297457            0.243199            0.213202   \n",
       "\n",
       "   split17_test_score  split18_test_score  split19_test_score  \n",
       "0            0.169351            0.162488            0.132780  \n",
       "1            0.174804            0.162162            0.123704  \n",
       "2            0.182748            0.162867            0.120160  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(clf.cv_results_)\n",
    "# extract all k=20 step\n",
    "result = result.iloc[:,result.columns.str.contains('split')]\n",
    "result.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction of the confidence interval for each split\n",
    "\n",
    "confidence_interval=[]\n",
    "for i in range(result.shape[0]):\n",
    "    ci = sorted(result.loc[i,:])\n",
    "    confidence_interval.append([ci[1],ci[18]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the mean  𝑅2  along with the 90% CI for the 18 scenarios. Notice that you can calculate the 90% CI in a bootstrap-like fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 0 : \n",
      "\n",
      " The mean R2 is equal to 0.20237224797904046 with a 90% CI equal to [0.09307305582617853, 0.2862114689320536]. \n",
      "\n",
      "Scenario 1 : \n",
      "\n",
      " The mean R2 is equal to 0.1974256597292519 with a 90% CI equal to [0.09125241712184895, 0.27701918816167326]. \n",
      "\n",
      "Scenario 2 : \n",
      "\n",
      " The mean R2 is equal to 0.19316718256534904 with a 90% CI equal to [0.0930853873404559, 0.27196817468543455]. \n",
      "\n",
      "Scenario 3 : \n",
      "\n",
      " The mean R2 is equal to 0.1842393235039952 with a 90% CI equal to [0.0727784232351697, 0.2606634427708543]. \n",
      "\n",
      "Scenario 4 : \n",
      "\n",
      " The mean R2 is equal to 0.17669635181548746 with a 90% CI equal to [0.06918090069824467, 0.2766145988969331]. \n",
      "\n",
      "Scenario 5 : \n",
      "\n",
      " The mean R2 is equal to 0.16915985678225698 with a 90% CI equal to [0.05934833560885966, 0.27022819984885227]. \n",
      "\n",
      "Scenario 6 : \n",
      "\n",
      " The mean R2 is equal to 0.20106269914436153 with a 90% CI equal to [0.1022047315691258, 0.27254368982077337]. \n",
      "\n",
      "Scenario 7 : \n",
      "\n",
      " The mean R2 is equal to 0.2055827246480971 with a 90% CI equal to [0.09857211166559698, 0.2841243467943312]. \n",
      "\n",
      "Scenario 8 : \n",
      "\n",
      " The mean R2 is equal to 0.20363513039464154 with a 90% CI equal to [0.09300908432308341, 0.28254457784656883]. \n",
      "\n",
      "Scenario 9 : \n",
      "\n",
      " The mean R2 is equal to 0.20019995807464755 with a 90% CI equal to [0.09101593355208049, 0.2795031178224251]. \n",
      "\n",
      "Scenario 10 : \n",
      "\n",
      " The mean R2 is equal to 0.19471187072880194 with a 90% CI equal to [0.08972469380016879, 0.2670980639748528]. \n",
      "\n",
      "Scenario 11 : \n",
      "\n",
      " The mean R2 is equal to 0.19082712071941735 with a 90% CI equal to [0.08380269236495963, 0.2629693179960717]. \n",
      "\n",
      "Scenario 12 : \n",
      "\n",
      " The mean R2 is equal to 0.11107733397782782 with a 90% CI equal to [0.08094627893163941, 0.14627866411925217]. \n",
      "\n",
      "Scenario 13 : \n",
      "\n",
      " The mean R2 is equal to 0.14036700124745644 with a 90% CI equal to [0.10114253789787109, 0.18483775486847542]. \n",
      "\n",
      "Scenario 14 : \n",
      "\n",
      " The mean R2 is equal to 0.1603554072097629 with a 90% CI equal to [0.1074174369301295, 0.21339323840898317]. \n",
      "\n",
      "Scenario 15 : \n",
      "\n",
      " The mean R2 is equal to 0.18359356308801472 with a 90% CI equal to [0.11272383335274117, 0.246128130170717]. \n",
      "\n",
      "Scenario 16 : \n",
      "\n",
      " The mean R2 is equal to 0.1943520955779066 with a 90% CI equal to [0.10760410213380911, 0.260852887010911]. \n",
      "\n",
      "Scenario 17 : \n",
      "\n",
      " The mean R2 is equal to 0.20025593844009246 with a 90% CI equal to [0.10476411673807973, 0.27301223260640195]. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "R2_mean = result.mean(axis=1)\n",
    "# number of scenario\n",
    "scenarios = np.arange(18)\n",
    "\n",
    "#printing result\n",
    "for scenario in scenarios:\n",
    "    print('Scenario {0} : \\n\\n The mean R2 is equal to {1} with a 90% CI equal to {2}. \\n'.format(\n",
    "        scenario,R2_mean[scenario],confidence_interval[scenario]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R2_mean</th>\n",
       "      <th>confidence_interval</th>\n",
       "      <th>lower_conf_int</th>\n",
       "      <th>upper_conf_int</th>\n",
       "      <th>lower_err</th>\n",
       "      <th>upper_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.202372</td>\n",
       "      <td>[0.09307305582617853, 0.2862114689320536]</td>\n",
       "      <td>0.093073</td>\n",
       "      <td>0.286211</td>\n",
       "      <td>0.109299</td>\n",
       "      <td>0.083839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.197426</td>\n",
       "      <td>[0.09125241712184895, 0.27701918816167326]</td>\n",
       "      <td>0.091252</td>\n",
       "      <td>0.277019</td>\n",
       "      <td>0.106173</td>\n",
       "      <td>0.079594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.193167</td>\n",
       "      <td>[0.0930853873404559, 0.27196817468543455]</td>\n",
       "      <td>0.093085</td>\n",
       "      <td>0.271968</td>\n",
       "      <td>0.100082</td>\n",
       "      <td>0.078801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.184239</td>\n",
       "      <td>[0.0727784232351697, 0.2606634427708543]</td>\n",
       "      <td>0.072778</td>\n",
       "      <td>0.260663</td>\n",
       "      <td>0.111461</td>\n",
       "      <td>0.076424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.176696</td>\n",
       "      <td>[0.06918090069824467, 0.2766145988969331]</td>\n",
       "      <td>0.069181</td>\n",
       "      <td>0.276615</td>\n",
       "      <td>0.107515</td>\n",
       "      <td>0.099918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.169160</td>\n",
       "      <td>[0.05934833560885966, 0.27022819984885227]</td>\n",
       "      <td>0.059348</td>\n",
       "      <td>0.270228</td>\n",
       "      <td>0.109812</td>\n",
       "      <td>0.101068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.201063</td>\n",
       "      <td>[0.1022047315691258, 0.27254368982077337]</td>\n",
       "      <td>0.102205</td>\n",
       "      <td>0.272544</td>\n",
       "      <td>0.098858</td>\n",
       "      <td>0.071481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.205583</td>\n",
       "      <td>[0.09857211166559698, 0.2841243467943312]</td>\n",
       "      <td>0.098572</td>\n",
       "      <td>0.284124</td>\n",
       "      <td>0.107011</td>\n",
       "      <td>0.078542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.203635</td>\n",
       "      <td>[0.09300908432308341, 0.28254457784656883]</td>\n",
       "      <td>0.093009</td>\n",
       "      <td>0.282545</td>\n",
       "      <td>0.110626</td>\n",
       "      <td>0.078909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.200200</td>\n",
       "      <td>[0.09101593355208049, 0.2795031178224251]</td>\n",
       "      <td>0.091016</td>\n",
       "      <td>0.279503</td>\n",
       "      <td>0.109184</td>\n",
       "      <td>0.079303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.194712</td>\n",
       "      <td>[0.08972469380016879, 0.2670980639748528]</td>\n",
       "      <td>0.089725</td>\n",
       "      <td>0.267098</td>\n",
       "      <td>0.104987</td>\n",
       "      <td>0.072386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.190827</td>\n",
       "      <td>[0.08380269236495963, 0.2629693179960717]</td>\n",
       "      <td>0.083803</td>\n",
       "      <td>0.262969</td>\n",
       "      <td>0.107024</td>\n",
       "      <td>0.072142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.111077</td>\n",
       "      <td>[0.08094627893163941, 0.14627866411925217]</td>\n",
       "      <td>0.080946</td>\n",
       "      <td>0.146279</td>\n",
       "      <td>0.030131</td>\n",
       "      <td>0.035201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.140367</td>\n",
       "      <td>[0.10114253789787109, 0.18483775486847542]</td>\n",
       "      <td>0.101143</td>\n",
       "      <td>0.184838</td>\n",
       "      <td>0.039224</td>\n",
       "      <td>0.044471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.160355</td>\n",
       "      <td>[0.1074174369301295, 0.21339323840898317]</td>\n",
       "      <td>0.107417</td>\n",
       "      <td>0.213393</td>\n",
       "      <td>0.052938</td>\n",
       "      <td>0.053038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.183594</td>\n",
       "      <td>[0.11272383335274117, 0.246128130170717]</td>\n",
       "      <td>0.112724</td>\n",
       "      <td>0.246128</td>\n",
       "      <td>0.070870</td>\n",
       "      <td>0.062535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.194352</td>\n",
       "      <td>[0.10760410213380911, 0.260852887010911]</td>\n",
       "      <td>0.107604</td>\n",
       "      <td>0.260853</td>\n",
       "      <td>0.086748</td>\n",
       "      <td>0.066501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.200256</td>\n",
       "      <td>[0.10476411673807973, 0.27301223260640195]</td>\n",
       "      <td>0.104764</td>\n",
       "      <td>0.273012</td>\n",
       "      <td>0.095492</td>\n",
       "      <td>0.072756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     R2_mean                         confidence_interval  lower_conf_int  \\\n",
       "0   0.202372   [0.09307305582617853, 0.2862114689320536]        0.093073   \n",
       "1   0.197426  [0.09125241712184895, 0.27701918816167326]        0.091252   \n",
       "2   0.193167   [0.0930853873404559, 0.27196817468543455]        0.093085   \n",
       "3   0.184239    [0.0727784232351697, 0.2606634427708543]        0.072778   \n",
       "4   0.176696   [0.06918090069824467, 0.2766145988969331]        0.069181   \n",
       "5   0.169160  [0.05934833560885966, 0.27022819984885227]        0.059348   \n",
       "6   0.201063   [0.1022047315691258, 0.27254368982077337]        0.102205   \n",
       "7   0.205583   [0.09857211166559698, 0.2841243467943312]        0.098572   \n",
       "8   0.203635  [0.09300908432308341, 0.28254457784656883]        0.093009   \n",
       "9   0.200200   [0.09101593355208049, 0.2795031178224251]        0.091016   \n",
       "10  0.194712   [0.08972469380016879, 0.2670980639748528]        0.089725   \n",
       "11  0.190827   [0.08380269236495963, 0.2629693179960717]        0.083803   \n",
       "12  0.111077  [0.08094627893163941, 0.14627866411925217]        0.080946   \n",
       "13  0.140367  [0.10114253789787109, 0.18483775486847542]        0.101143   \n",
       "14  0.160355   [0.1074174369301295, 0.21339323840898317]        0.107417   \n",
       "15  0.183594    [0.11272383335274117, 0.246128130170717]        0.112724   \n",
       "16  0.194352    [0.10760410213380911, 0.260852887010911]        0.107604   \n",
       "17  0.200256  [0.10476411673807973, 0.27301223260640195]        0.104764   \n",
       "\n",
       "    upper_conf_int  lower_err  upper_err  \n",
       "0         0.286211   0.109299   0.083839  \n",
       "1         0.277019   0.106173   0.079594  \n",
       "2         0.271968   0.100082   0.078801  \n",
       "3         0.260663   0.111461   0.076424  \n",
       "4         0.276615   0.107515   0.099918  \n",
       "5         0.270228   0.109812   0.101068  \n",
       "6         0.272544   0.098858   0.071481  \n",
       "7         0.284124   0.107011   0.078542  \n",
       "8         0.282545   0.110626   0.078909  \n",
       "9         0.279503   0.109184   0.079303  \n",
       "10        0.267098   0.104987   0.072386  \n",
       "11        0.262969   0.107024   0.072142  \n",
       "12        0.146279   0.030131   0.035201  \n",
       "13        0.184838   0.039224   0.044471  \n",
       "14        0.213393   0.052938   0.053038  \n",
       "15        0.246128   0.070870   0.062535  \n",
       "16        0.260853   0.086748   0.066501  \n",
       "17        0.273012   0.095492   0.072756  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plotting\n",
    "d ={'R2_mean': R2_mean ,'confidence_interval':confidence_interval}\n",
    "plot = pd.DataFrame(data=d)\n",
    "plot['lower_conf_int'] =plot['confidence_interval'].apply(lambda x : x[0])\n",
    "plot['upper_conf_int'] =plot['confidence_interval'].apply(lambda x : x[1])\n",
    "plot['lower_err'] = plot['R2_mean'] - plot['lower_conf_int']\n",
    "plot['upper_err'] = plot['upper_conf_int'] - plot['R2_mean']\n",
    "#plotting result\n",
    "#plt.errorbar(plot.index,plot['R2_mean'],y_err=[plot['lower_err'],plot['upper_err']])\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.5.3**\n",
    "With the best hyper-parameters obtained, train your model with the entire training set and report the $R^2$ on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model for our best parameter\n",
    "\n",
    "optimal_reg = ensemble.GradientBoostingRegressor(**best_param).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on testing set =  0.22111889997778844\n"
     ]
    }
   ],
   "source": [
    "# prediction and R2\n",
    "pred = optimal_reg.predict(X_test)\n",
    "r2 = metrics.r2_score(y_test,pred)\n",
    "print('R2 on testing set = ',r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.5.4**\n",
    "#### **Discussion:** \n",
    "Why don't we tune the hyper-parameters in the testing set instead of doing cross-validation in the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better representation of the reality, the hyper-parameters must be only train on the train set. If we train the hyper-parameters on the test set we can have a model perfectly representative of our test set but that behave poorly with other data. In order to avoid that, we used the cross  k  validation on the train test in order to get a good estimation of the true loss and the true accuracy of our model. The model is also much more robust to other data with same features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Step 2:_ What influences papers getting accepted?\n",
    "\n",
    "Time to change hats!\n",
    "\n",
    "If before we were interested in creating an accurate regressor, now we are interested in understanding what increases the chance of papers getting accepted. \n",
    "\n",
    "Typically, in that scenario, simpler models with a clear statistical interpretation (e.g. logistic regression) yield more interesting insights.\n",
    "\n",
    "For the analysis in this and the next step, you should use [statsmodels](https://www.statsmodels.org/) (for the regressions) and [scipy](https://www.scipy.org/) (for the statistical hypothesis testing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "\n",
    "Let's warm up with some visualizations and some hypothesis testing!\n",
    "\n",
    "1. Plot the distributions of 1) ratings of papers that got accepted in 2020, 2) ratings of papers that got rejected in 2020.\n",
    "2. Select a statistical test to compare whether the mean for ratings of papers that got accepted in 2020 is significantly higher.\n",
    "3. **Discuss:** Justify why the statistical test you selected is appropriate. Interpret the test-related statistic and its p-value: concretely, what do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accept_2020 = df[(df['year'] == 2020)&(df['decisions'] == 'Accept')]['ratings']\n",
    "reject_2020 = df[(df['year'] == 2020)&(df['decisions'] == 'Reject')]['ratings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_2020.hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reject_2020.hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_2020.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_2020.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_2020.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_2020.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(accept_2020, reject_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "\n",
    "We will now carry out a logistic regression modeling the binary variable `decisions` as a function of the continuous variable `ratings` and an intercept. \n",
    "\n",
    "Recall that a logistic regression is a model in the form:\n",
    "\n",
    "$$\n",
    "\\log \\Big( \\frac{p}{1-p} \\Big) = b_0 + b_1x_1 + b_2x_2 + \\ldots\n",
    "$$\n",
    "\n",
    "Where $p$ is the probability of the dependent variable (here, `decisions`) being equals to 1. \n",
    "Note that $\\frac{p}{1-p}$ represents the odds of the variable, and thus, on the left-hand side we have the log-odds of the variable.\n",
    "This can be also written as:\n",
    "\n",
    "$$\n",
    "\\Big( \\frac{p}{1-p} \\Big) = e^{b_0 + b_1 x_1 + b_2 x_2 + \\ldots} =  e^{b_0} e^{b_1 x_1} e^{b_2 x_2} \\ldots\n",
    "$$\n",
    "\n",
    "Given a linear variable, say $x_1$, if we increase the value associated with this variable by a single unit, and keep everything constant, we have:\n",
    "\n",
    "$$\n",
    " e^{b_1(x_1+1)} =  e^{b_1 x_1 + b_1} = e^{b_1 x_1}e^{b_1}  \n",
    "$$\n",
    "\n",
    "This means that we multiply the odds of the outcome variable by $e^{b_1}$. Thus, let's say that $x_1$ is the average rating, and $\\beta_1$ is the associated coefficient. Also, let's assume that $\\beta_1$ equals 2. In that case, increasing the score of the paper by 1 unit is equivalent to multiplying both sides of the previous equation by $e^{b_1}$. The original equation for our model becomes:\n",
    "\n",
    "$$\n",
    "\\Big( \\frac{p}{1-p} \\Big) = e^{b_0} e^{b_1 x_1} \n",
    "$$\n",
    "$$\n",
    "\\Big( \\frac{p}{1-p} \\Big) e^{b_1}  = e^{b_0} e^{b_1 x_1} e^{b_1}\n",
    "$$\n",
    "\n",
    "Since $b_1=2$, we have that this is the same as multiplying the odds of the variable by $e^2\\approx7$. \n",
    "So for example, if a paper with a score 5 had $p=0.05$ of being approved, its odds would be $0.05/0.95\\approx0.052$. According to our model, an increase in 1-rating point would mean that the new odds would be $0.052*7\\approx0.36$. Using the odds formula  ($\\frac{p}{1-p}$), this suggests that this paper would have a chance of $0.56$ of being accepted.\n",
    "\n",
    "---\n",
    "\n",
    "This is the theory. Now, let's find out what the real world looks like.\n",
    "\n",
    "\n",
    "1. Fit a logistic regression model to our data considering as the training set all papers submitted in 2020. Your model should predict a binary variable related to decisions (which equals true if the paper was accepted and false otherwise) as a function of the paper ratings and an intercept. In mathematical notation:\n",
    "$$\n",
    "d = b_0 + r b_1\n",
    "$$\n",
    "Where $d$ is the binary variable corresponding to a decision, $r$ is a numeric variable corresponding to the rating a paper has received and $b$ are coefficients.\n",
    "Notice that here we have no testing set!\n",
    "Report the summary of your model. \n",
    "2. **Discuss:** Interpreting the coefficients of your model, calculate the probability that a paper with a score 7 will be accepted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decisions(x):\n",
    "    if x == 'Accept':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['decisions'] = df['decisions'].apply(decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20 = df[df['year'] == 2020]\n",
    "X = df_20['ratings']\n",
    "X = sm.add_constant(X)\n",
    "y = df_20['decisions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = sm.Logit(y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.predict([1,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "\n",
    "Our model is not so interesting since the only independent variable (that is, our model' feature or predictor) is something blatantly associated with paper acceptance. Let's go further! \n",
    "\n",
    "1. Run a logistic regression with the binary decision as the dependent variable (the outcome) and using as independent variables (the features): ratings, the reputation of the last author, and whether the paper was on arxiv or not. That is, in [patsy-style formula](https://patsy.readthedocs.io/en/latest/formulas.html): `decisions ~ ratings + reputation + arxiv` (variable names do not need to be exactly these). Consider all papers submitted in 2020 as your training data.\n",
    "Notice that reputation was calculated in Task 1.1. \n",
    "2. **Discuss:** Unlike `ratings` and `reputation`, the variable `arxiv` is binary. Following the same logic as we did for continuous variables in the text of Task 2.2, interpret the meaning of this coefficient. What happens to the odds ratio if the paper was seen in arxiv? Is this effect statistically significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20['reputation'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "formula = 'decisions ~ ratings + reputation + C(arxiv_True)'\n",
    "log_2 = sm.Logit.from_formula(formula,data = df_20).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_2.predict([1,7,1.2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4\n",
    "\n",
    "Let's finally move on to the question that you have been dying to answer: Do pre-prints favor top institutions? \n",
    "\n",
    "In order to (try to) answer that question, you must fit yet another logistic regression.\n",
    "\n",
    "Your regression should have the paper decision as the dependent variable, and `ratings`, `reputation`, and `arxiv` as independent variables just like in task 2.3. Yet, here, include also as independent variables the binary variable `has_top_institution`, which equals 1 if the paper has an author in a top-10 institution; and the interaction variable `arxiv:has_top_institution`, which equals one only if the paper is from a top-10 institution **and** if it appeared on arxiv before the submission deadline. In patsy-style formula your model should look something like: \n",
    "\n",
    "`decisions_bool ~ ratings + reputation + arxiv + has_top_institution + arxiv:has_top_institution`\n",
    "\n",
    "\n",
    "1. Fit this model and estimate the effect of posting on arxiv for a top institution. Again, consider only papers submitted in the 2020 edition.\n",
    "2. **Discuss:** Interpreting the p-values, discuss: is this evidence that arxiv breaks double-blind submissions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'decisions ~ ratings + reputation + C(arxiv_True) + C(has_top_institution) + arxiv_True:has_top_institution'\n",
    "favor = sm.Logit.from_formula(formula,data = df_20).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Matching\n",
    "\n",
    "Okay, so let's change hats one last time.\n",
    "\n",
    "As you have all seen in class, a particularly powerful way of disentangling causal effects from observational data is through matching: making sure we are not comparing apples to oranges.\n",
    "\n",
    "\n",
    "### Task 3.1\n",
    "\n",
    "One of the ways to do matching is called \"Propensity Score Matching.\" There, we calculate a *propensity score* for each subject which represents the propensity to receive a \"treatment.\" Then, we match subjects who received and did not receive the treatment, but that had similar propensity scores (that is, even though some received the treatment and some did not, they had similar chances to receive it).\n",
    "\n",
    "In our specific case, we can further attempt to study the impact of pre-prints by considering publishing to arxiv as our \"treatment\".\n",
    "\n",
    "In that context, the first step to perform propensity score matching is to create a classifier that predicts whether a paper was published on arxiv or not.\n",
    "\n",
    "1. We have trained this classifier for you (don't get spoiled!). You may load the pandas dataframe entitled `propensity_scores.csv.gz` from the github repo. \n",
    "Notice that we will be using only the articles published in 2020. \n",
    "Create a new dataframe that is a merged version of this new dataframe with the dataframe you have been working on so far. Remember to keep only the papers published in 2020!\n",
    "2. **Discuss:** In which way is this classifier (that is a classifier trained to estimate propensity scores) different from the previous classifier that you trained in Step 1?\n",
    "\n",
    "**Hint:** For a classifier tra we don't need a training set and a test set!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROP_PATH = './Data/propensity_scores.csv.gz'\n",
    "propensity_df = pd.read_csv(PROP_PATH, compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_20 = df_20.merge(propensity_df, on = 'paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2\n",
    "\n",
    "Now is time to match users! You will be implementing a technique called *caliper matching*. \n",
    "\n",
    "Create pairs of users as follows. For each treated subject (here a paper that was submitted to arxiv), find a non-treated subject (a paper that was not submitted to arxiv) with similar propensity score. Namely, if the propensity score of the treated unit is $p_t$, you must find a non-treated unit $p_c$ with propensity score $|p_{c} - p_t| < \\epsilon$. If there is no such a non-treated unit, you may ignore the treated unit and move forward with the matching. Notice that your output must be a 1-to-1 matching. So each paper can only be matched once.\n",
    "\n",
    "1. Perform this matching procedure using $\\epsilon = 0.05$.\n",
    "2. Report how many pairs did you manage to match? How many didn't you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_20_not['propensity_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_20_arxiv = prop_20[prop_20['arxiv_True'] == 1] \n",
    "prop_20_not = prop_20[prop_20['arxiv_True'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nx.Graph()\n",
    "B.add_nodes_from(list(prop_20_arxiv.index), bipartite = 0)\n",
    "B.add_nodes_from(list(prop_20_not.index), bipartite = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_arxiv = prop_20_arxiv['propensity_score'].to_dict()\n",
    "graph_not = prop_20_not['propensity_score'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = .05\n",
    "for key,value in graph_arxiv.items():\n",
    "    for key_2,value_2 in graph_not.items():\n",
    "        if np.abs(value-value_2) <= E:\n",
    "            B.add_edge(key, key_2, weight = np.abs(value-value_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite.is_bipartite(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in bipartite.generate_edgelist(B):\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prop_20_arxiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = bipartite.minimum_weight_full_matching(B,list(prop_20_arxiv.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_20_matched = prop_20_arxiv.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_20_matched['match'] = prop_20_arxiv.index.map(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_20_matched = prop_20_matched.merge(prop_20_not, right_index = True, left_on = 'match', suffixes = ['_treat','_not'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3.3\n",
    "\n",
    "Let's evaluate your matching! There are more rigorous ways to do it, but we'll go the easy path.\n",
    "\n",
    "\n",
    "1. Using one or several appropriate plot types, visualize the distributions of the variables reputation and ratings, for treated and non-treated subjects that you matched.\n",
    "2. Visualize the distributions of these variables for all papers from 2020 that appeared on arxiv, and all papers that did not (including those you did not manage to match).\n",
    "3. **Discuss:** According to your visual analysis (no statistical tests needed here), did the matching do a good job at balancing covariates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,figsize=(10, 3), sharex = True, sharey = True)\n",
    "axs.scatter(prop_20_matched['ratings_treat'],prop_20_matched['ratings_not'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,figsize=(10, 3), sharex = True, sharey = True)\n",
    "axs.scatter(prop_20_matched['reputation_treat'],prop_20_matched['reputation_not'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,1,figsize=(10, 3), sharex = True, sharey = True)\n",
    "sns.boxplot(x = prop_20_matched['ratings_treat'], ax = axs[0])\n",
    "sns.boxplot(x = prop_20_matched['ratings_not'], ax = axs[1])\n",
    "sns.boxplot(x = prop_20_not['ratings'], ax = axs[2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,1,figsize=(10, 3), sharex = True, sharey = True)\n",
    "sns.boxplot(x = prop_20_matched['reputation_treat'], ax = axs[0])\n",
    "sns.boxplot(x = prop_20_matched['reputation_not'], ax = axs[1])\n",
    "sns.boxplot(x = prop_20_not['reputation'], ax = axs[2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(10, 3), sharex = True, sharey = True)\n",
    "axs[0].hist(prop_20_matched['ratings_treat'], bins = 10, density = True)\n",
    "axs[1].hist(prop_20_matched['ratings_not'], bins = 10, density = True)\n",
    "axs[2].hist(prop_20_not['ratings'], bins = 10, density = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(10, 3), sharex = True, sharey = True)\n",
    "axs[0].hist(prop_20_matched['reputation_treat'], bins = 15, density = True)\n",
    "axs[1].hist(prop_20_matched['reputation_not'], bins = 15, density = True)\n",
    "axs[2].hist(prop_20_not['reputation'], bins = 15, density = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(10, 3), sharex = True, sharey = True)\n",
    "axs[0].hist(prop_20_matched['ratings_treat'], bins = 15, density = True)\n",
    "axs[1].hist(prop_20_not['ratings'], bins = 15, density = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(10, 3), sharex = True, sharey = True)\n",
    "axs[0].hist(prop_20_matched['reputation_treat'], bins = 15, density = True)\n",
    "axs[1].hist(prop_20_not['reputation'], bins = 15, density = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3.4\n",
    "\n",
    "Finally, it's time to estimate the treatment effect. Given that we have pairs of matched samples, one that received treatment and one that did not, and that they have associated outcomes $y_{treat}$ and $y_{\\neg treat}$. We can calculate the average treatment effect as:\n",
    "\n",
    "$$\n",
    "ATE = \\frac{1}{N} \\sum_i^N  y_{treat}^{(i)} - y_{\\neg treat}^{(i)}\n",
    "$$\n",
    "\n",
    "Notice that here the outcome is a simple binary variable which equals 1 if the paper has been accepted and equals 0 if the paper has been rejected, and $N$ is the total number of matched samples in our analysis.\n",
    "\n",
    "According to your matched sample, estimate the treatment effect of publishing a paper on arxiv. Report the 95% CI through bootstrapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iters = 1\n",
    "ate = []\n",
    "for i in range(iters):\n",
    "    sample = prop_20_matched.sample(frac = 1, replace = False)\n",
    "    ate.append(np.sum(sample['decisions_treat'] - sample['decisions_not'])/sample.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sample['decisions_treat'] - sample['decisions_not'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sample['decisions_treat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sample['decisions_not'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3.5\n",
    "\n",
    "Wait, but what about prestigious institutions? \n",
    "\n",
    "1.  To understand what is going on there, repeat tasks 3.2 to 3.4 considering *only* the top 10 institutions. Notice that you can use the same propensity scores and re-do the steps in a reduced dataframe containing only top-institutions.\n",
    "2. **Discuss:** Is this evidence that arxiv breaks double-blind submissions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Your code here! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
